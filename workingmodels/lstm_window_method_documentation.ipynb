{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a LSTM neural network for Time Series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window with multi-step forecasting method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This LSTM model was initially tested using data from WTW. However, in this presentation, I used dummy data randomly generated. No data saved from WTW has been used here and any coincidence is a casualty.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>14895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>16558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>17042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>17821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Sales\n",
       "0  2017      1  15000\n",
       "1  2017      2  14895\n",
       "2  2017      3  16558\n",
       "3  2017      4  17042\n",
       "4  2017      5  17821"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test1.csv', header=0, sep=',')\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Before jumping directly into the code. We will explore how the code is built so you can better understand each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly while I was researching the best possible approach for building a LSTM neural network for Time Series forecasting many of the trusted sources consulted (books and websites) was by building window time within the dataset. This allows the algorithm to better understand periods of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining what is window time and how it works. We need to reshape the existing dataset. Instead of having year and month separated we will reshape into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>14895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>16558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>17042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>17821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Year_Month  Sales\n",
       "0 2017-01-01  15000\n",
       "1 2017-02-01  14895\n",
       "2 2017-03-01  16558\n",
       "3 2017-04-01  17042\n",
       "4 2017-05-01  17821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m')\n",
    "dataset = read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test1.csv',\n",
    "                   parse_dates = [['Year', 'Month']],\n",
    "                   header=0,\n",
    "                   date_parser=parse)\n",
    "dataset.head(5)\n",
    "dataset.to_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test-parse.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window With Multi-Step Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sequence of numbers for a time series dataset, we can restructure the data to look like a supervised learning problem. We can do this by using previous time steps as input variables and use the next time step as the output variable.\n",
    "Let’s make this concrete with an example. Imagine we have a time series as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2017-01-01  15000\n",
    "# 2017-02-01  14895\n",
    "# 2017-03-01  16558\n",
    "# 2017-04-01  17042\n",
    "# 2017-05-01  17821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restructure this time series dataset as a supervised learning problem by using the value at the previous time step to predict the value at the next time-step. Re-organizing the time series dataset this way, the data would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   x      y\n",
    "#   ?    15000\n",
    "# 15000  14895\n",
    "# 14895  16558\n",
    "# 16558  17042\n",
    "# 17042  17042\n",
    "# 17821    ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some observations:\n",
    "\n",
    "- We can see that the previous time step is the input (X) and the next time step is the output (y) in our supervised learning problem.\n",
    "- We can see that the order between the observations is preserved, and must continue to be preserved when using this dataset to train a supervised model.\n",
    "- We can see that we have no previous value that we can use to predict the first value in the sequence. We will delete this row as we cannot use it.\n",
    "- We can also see that we do not have a known next value to predict for the last value in the sequence. We may want to delete this value while training our supervised model also.\n",
    "\n",
    "The use of prior time steps to predict the next time step is called the sliding window method. For short, it may be called the window method in some literature. In statistics and time series analysis, this is called a lag or lag method. The number of previous time steps is called the window width or size of the lag. Careful thought and experimentation are needed on your problem to find a window width that results in acceptable model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The window_method() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the shift() function in Pandas to automatically create new framings of time series problems given the desired length of input and output sequences.\n",
    "\n",
    "This would be a useful tool as it would allow us to explore different framings of a time series problem with machine learning algorithms to see which might result in better performing models.\n",
    "\n",
    "In this section, we will define a new Python function named window_method() that takes a univariate or multivariate time series and frames it as a supervised learning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes four arguments:\n",
    "- data: Sequence of observations as a list or 2D NumPy array. Required.\n",
    "- lookBack: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.\n",
    "- delay: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.\n",
    "- dropnan: Boolean whether or not to drop rows with NaN values. Optional. Defaults to True.\n",
    "\n",
    "The function returns a single value:\n",
    "- return: Pandas DataFrame of series framed for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-1)  var2(t-1)     var1(t)  var2(t)\n",
      "1   2017-01-01    15000.0  2017-02-01    14895\n",
      "2   2017-02-01    14895.0  2017-03-01    16558\n",
      "3   2017-03-01    16558.0  2017-04-01    17042\n",
      "4   2017-04-01    17042.0  2017-05-01    17821\n",
      "5   2017-05-01    17821.0  2017-06-01    18600\n",
      "6   2017-06-01    18600.0  2017-07-01    19379\n",
      "7   2017-07-01    19379.0  2017-08-01    20158\n",
      "8   2017-08-01    20158.0  2017-09-01    20937\n",
      "9   2017-09-01    20937.0  2017-10-01    21716\n",
      "10  2017-10-01    21716.0  2017-11-01    22495\n",
      "11  2017-11-01    22495.0  2017-12-01    23274\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def window_method(data, lookBack=1, delay=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(lookBack, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, delay):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test-parse.csv', header=0, sep=',')\n",
    "df.values\n",
    "data = window_method(df)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we've seen how to reframe dataset using shift() function to convert into one-step or multi-step supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code. We take as an example, look back 1 month and try to predict the following 3 month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15000. 14895. 16558. 17042.]\n",
      " [14895. 16558. 17042. 17821.]\n",
      " [16558. 17042. 17821. 18600.]\n",
      " [17042. 17821. 18600. 19379.]\n",
      " [17821. 18600. 19379. 20158.]\n",
      " [18600. 19379. 20158. 20937.]]\n",
      " \n",
      "look back: 15000.0 (t-1) delay: 14895.0 (t+1)\n",
      "look back: 15000.0 (t-1) delay: 16558.0 (t+2)\n",
      "look back: 15000.0 (t-1) delay: 17042.0 (t+3)\n",
      "look back: 14895.0 (t-1) delay: 16558.0 (t+1)\n",
      "look back: 14895.0 (t-1) delay: 17042.0 (t+2)\n",
      "look back: 14895.0 (t-1) delay: 17821.0 (t+3)\n",
      "look back: 16558.0 (t-1) delay: 17042.0 (t+1)\n",
      "look back: 16558.0 (t-1) delay: 17821.0 (t+2)\n",
      "look back: 16558.0 (t-1) delay: 18600.0 (t+3)\n",
      "look back: 17042.0 (t-1) delay: 17821.0 (t+1)\n",
      "look back: 17042.0 (t-1) delay: 18600.0 (t+2)\n",
      "look back: 17042.0 (t-1) delay: 19379.0 (t+3)\n",
      "look back: 17821.0 (t-1) delay: 18600.0 (t+1)\n",
      "look back: 17821.0 (t-1) delay: 19379.0 (t+2)\n",
      "look back: 17821.0 (t-1) delay: 20158.0 (t+3)\n",
      "look back: 18600.0 (t-1) delay: 19379.0 (t+1)\n",
      "look back: 18600.0 (t-1) delay: 20158.0 (t+2)\n",
      "look back: 18600.0 (t-1) delay: 20937.0 (t+3)\n",
      "None\n",
      " \n",
      "Train: (6, 4), Test: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def window_method(data, lookBack=1, delay=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(lookBack, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, delay):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, test_samples, lookBack, delay):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = window_method(raw_values, lookBack, delay)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-test_samples], supervised_values[-test_samples:]\n",
    "    return train, test\n",
    "\n",
    "def reference_number_print(train):\n",
    "    for row in range(train.shape[0]):\n",
    "        for column in range(delay):\n",
    "            print('look back:', train[row][0], '(t-1)', 'delay:', train[row][column+1], '(t+'+str(column+1)+')')\n",
    "    \n",
    "# load dataset\n",
    "series = read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test-parse.csv', header=0, index_col=0, sep=',')\n",
    "# configure\n",
    "lookBack = 1\n",
    "delay = 3\n",
    "test_samples = 3\n",
    "# prepare data\n",
    "train, test = prepare_data(series, test_samples, lookBack, delay)\n",
    "print(train)\n",
    "print(' ')\n",
    "ref = reference_number_print(train)\n",
    "print(ref)\n",
    "print(' ')\n",
    "print('Train: %s, Test: %s' % (train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dummy dataset we generated randomly. The first 15000 relates to 2017-01-01 while the next 3 numbers are the following 3 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model is univariate (only 1 input) we will use Sequential() model from Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Time Series to Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other neural networks, LSTMs expect data to be within the scale of the activation function used by the network.\n",
    "\n",
    "The default activation function for LSTMs is the hyperbolic tangent (tanh), which outputs values between -1 and 1. This is the preferred range for the time series data. To make the experiment fair, the scaling coefficients (min and max) values must be calculated on the training dataset and applied to scale the test dataset and any forecasts. This is to avoid contaminating the experiment with knowledge from the test dataset, which might give the model a small edge.\n",
    "We can transform the dataset to the range [-1, 1] using the MinMaxScaler class. Like other scikit-learn transform classes, it requires data provided in a matrix format with rows and columns. Therefore, we must reshape our NumPy arrays before transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not changed dataset:\n",
      "            Sales\n",
      "Year_Month       \n",
      "2017-01-01  15000\n",
      "2017-02-01  14895\n",
      "2017-03-01  16558\n",
      "2017-04-01  17042\n",
      "2017-05-01  17821\n",
      " \n",
      "transformed dataset between range -1,1:\n",
      "0   -0.974937\n",
      "1   -1.000000\n",
      "2   -0.603055\n",
      "3   -0.487528\n",
      "4   -0.301587\n",
      "dtype: float64\n",
      " \n",
      "Inverted back the dataset to the original numbers:\n",
      "0    15000.0\n",
      "1    14895.0\n",
      "2    16558.0\n",
      "3    17042.0\n",
      "4    17821.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukle\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "series = read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test-parse.csv', header=0, index_col=0, sep=',')\n",
    "print(\"not changed dataset:\")\n",
    "print(series.head(5))\n",
    "\n",
    "def transform(series):\n",
    "    series = series.values                        # transform into numpy arrays\n",
    "    series = series.reshape(len(series), 1)       # reshape the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))  # range values to the transformed\n",
    "    scaler = scaler.fit(series)                   # see how to range values\n",
    "    scaled_serie = scaler.transform(series)       # apply changes to the values\n",
    "    scaled_series = Series(scaled_serie[:, 0])    # create a new series of data transformed\n",
    "    return scaled_series, scaler, scaled_serie\n",
    "\n",
    "inverted_array, scaler, inverted_serie = transform(series)\n",
    "print(\" \")\n",
    "print(\"transformed dataset between range -1,1:\")\n",
    "print(inverted_array.head(5))\n",
    "\n",
    "def invert_transform(inverted_array, scaler, inverted_serie):\n",
    "    inverted_serie = scaler.inverse_transform(inverted_serie)   # revert the scaler\n",
    "    inverted_series = Series(inverted_serie[:, 0])              # create a new series from reverted data\n",
    "    return inverted_series\n",
    "\n",
    "original_dataset = invert_transform(inverted_array, scaler, inverted_serie)\n",
    "print(\" \")\n",
    "print(\"Inverted back the dataset to the original numbers:\")\n",
    "print(original_dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red box is telling us that the funcion is changing from intengers (example: 1) to float (example: 1.0). It is expected as we are transforming values between -1 to 1.\n",
    "\n",
    "This changing values to smaller ranges also occurs when using other types of neural networks. This helps the neural network to understand more faster the numbers while having large ranges may take more time and training rounds for the neurons to understand and make relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code. The transformation of input values occurs at prepare_data() function and the invert transformation occurs after forecasting the number at inverse_transformation(). The scaler values (ranges of values used when transforming values) must need to be returned at prepare_data() and called at inverse_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, lookBack, delay):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(raw_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = window_method(scaled_values, lookBack, delay)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    "\n",
    "# process predictions\n",
    "def inverse_transform(series, forecasts, scaler, n_test):   # call back scaler from prepare_data()\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # store\n",
    "        inverted.append(inv_scale)\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM input reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\n",
    "\n",
    "The LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features].\n",
    "- Samples: These are independent observations from the domain, typically rows of data.\n",
    "- Time steps: These are separate time steps of a given variable for a given observation (for this dataset, each line is a time step).\n",
    "- Features: These are separate measures observed at the time of observation.\n",
    "\n",
    "This first requires that the training dataset be transformed from a 2D array [samples, features] to a 3D array [samples, timesteps, features]. We will fix time steps at 1, so this change is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:\n",
      "(12, 1)\n",
      "rows: 12 columns: 1\n",
      "Transformed shape for LSTM:\n",
      "(12, 1, 1)\n",
      "rows: 12 Time-Steps: 1 columns: 1\n"
     ]
    }
   ],
   "source": [
    "from pandas import *\n",
    "\n",
    "series = read_csv('C:\\\\Users\\\\mukle\\\\Desktop\\\\lstm\\\\test-parse.csv', header=0, index_col=0, sep=',')\n",
    "series = series.values  # transform to numpy array\n",
    "\n",
    "original = series.shape        # original shape, 2D array\n",
    "print(\"Original shape:\")\n",
    "print(original)\n",
    "print('rows:', original[0], 'columns:', original[-1])   \n",
    "\n",
    "print(\"Transformed shape for LSTM:\")\n",
    "transformed = series.reshape(series.shape[0], 1, series.shape[-1])    # changing the shape to 3D array\n",
    "print(transformed.shape)\n",
    "print('rows:', transformed.shape[0], 'Time-Steps:', transformed.shape[1], 'columns:', transformed.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code. Reshape occurs at fit_lstm() function. Once splitted the dataset between features and labels (x and y) the features (x) must need to be reshaped using the inbuilt function \"reshape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lstm(train, lookBack, delay, n_batch, n_epochs):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])    # Reshape input values to 3D Array\n",
    "    \n",
    "    \n",
    "    ...\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to design an LSTM network. We will use a simple structure with 1 hidden layer with 1 LSTM unit, then an output layer with linear activation and 3 output values (look back: 1 month, delay: 3 month). The network will use a mean squared error loss function and the efficient ADAM optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the input data must be specified in the LSTM layer using the “batch_input_shape” argument as a tuple that specifies the expected number of observations to read each batch, the number of time steps, and the number of features.\n",
    "The batch size is often much smaller than the total number of samples. It, along with the number of epochs, defines how quickly the network learns the data (how often the weights are updated).\n",
    "The final import parameter in defining the LSTM layer is the number of neurons, also called the number of memory units or blocks.\n",
    "\n",
    "The line below creates a single LSTM hidden layer that also specifies the expectations of the input layer via the “batch_input_shape” argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "layer = LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network requires a single neuron in the output layer with a linear activation to predict the number of shampoo sales at the next time step. Once the network is specified, it must be compiled into an efficient symbolic representation using a backend mathematical library, such as TensorFlow or Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True)) # LSTM neuron\n",
    "model.add(Dense(1)) # Single neuron which means that only has 1 outpuy\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code the model is designed at fit_lstm() where we also split the data, reshape and build the layers. The output layer takes the shape of \"y\". This makes us easy when we want more than 1 outcome, let's say we want to forecast three consecutive months. Therefore, we would have 3 output (3 neurons) that would predict t+1 (1 neuron), t+2 (1 neuron) and t+3 (1 neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lstm(neurons, train, lookBack, delay, n_batch, n_epochs, learning_rate):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:lookBack], train[:, lookBack:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate))\n",
    "    fit = model.fit(X, y, epochs=n_epochs, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "\n",
    "    loss = fit.history['loss']\n",
    "    epochs = range(1, n_epochs+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add more layers into the code, we simply have to add another layer defining the neuron units, batch input shape and if it returns the state of the network such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(10, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### - *Understanding stateful = True or stateful = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part can be difficult to understand. When to use stateful as true or false. The stateful setup could be quite difficult to grasp at first. One would expect the state to be transferred between the last sample of one batch to the first sample of the next batch. But the state is actually propagated across batches between the same numbered samples.\n",
    "\n",
    "By default, an LSTM layer in Keras maintains state between data within one batch (stateful = False). A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default. When we set stateful as true (stateful = True), in this mode the state is propagated from sample \"i\" of one batch to sample\"i\" of the next batch.\n",
    "\n",
    "The reason why by default keras set stateful as false (stateful = False) is because in large dataset after returning each state to the beginning of the next epoch and so on, it grows the state and it can be unstable. In small dataset resetting the state or keeping it for the next iteration shouldn't be an issue. But it is recommended to use both combinations to see which one performs better (in small dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you consider to reset the state after each iteration, you don't need to change to stateful = False. Simply changing the model.fit() by adding an internal loop (number of epochs to iterate) and after each iteration we would add model.reset_state() to reset the previous state and have no state for the next batch. The following code does reset the state after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the internal state is reset after each epoch. While forecasting, we will not want to reset the internal state between forecasts. In fact, we would like the model to build up state as we forecast each time step in the test dataset.\n",
    "This raises the question as to what would be a good initial state for the network prior to forecasting the test dataset.\n",
    "In our code, we will seed the state by making a prediction on all samples in the training dataset. In theory, the internal state should be set up ready to forecast the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the samples within an epoch are shuffled prior to being exposed to the network. Again, this is undesirable for the LSTM because we want the network to build up state as it learns across the sequence of observations. We can disable the shuffling of samples by setting “shuffle” to “False“.\n",
    "\n",
    "Also by default, the network reports a lot of debug information about the learning progress and skill of the model at the end of each epoch. We can disable this by setting the “verbose” argument to the level of “0“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, looking at the above code we set stateful as true. Which builds a state on each iteration during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting, dropout, recurrent dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no solution direct to overfitting. One of the main issues working with small datasets is the risk of overfitting is greater. To deal with overfitting Dropout/recurrent dropout/batch normalization layers may be useful. To add such layers we would have to do as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a forecast, we can call the predict() function on the model. This requires a 3D NumPy array input as an argument. In this case, it will be an array of one value, the observation at the previous time step.\n",
    "\n",
    "The predict() function returns an array of predictions, one for each input row provided. Because we are providing a single input, the output will be a 2D NumPy array with one value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time a new model is created it saves it automatically using the function model_save(). You may need to change the name but technically it won't overwrite an existing model because of random integer values is created on each time a new model is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_save(save_model_directory, model, lookBack, delay, n_test, n_epochs):\n",
    "    \"\"\"\n",
    "    Saves automatically each new model execution. Make sure to change:\n",
    "        - model_name: A new name for your model\n",
    "        # random_number: Generates a random number to make sure not to overwrite any existing model\n",
    "    \"\"\"\n",
    "    random_number = str(random.randint(1,99999999999))\n",
    "    model_stats = '_lookBack_'+str(lookBack)+'_delay_'+str(delay)+'_test_'+str(n_test)+'_epochs_'+str(n_epochs)\n",
    "    model_name = '_modelName_'+'changeName'\n",
    "    model_random_number = '_'+ random_number\n",
    "    model.save(save_model_directory+model_stats+model_name+model_random_number)\n",
    "    print(\"\\n model saved as:\", save_model_directory+model_stats+model_name+model_random_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to execute the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program has two build function: To execute a model, To load a model and To reformat de dataset.\n",
    "\n",
    "When the program executes it will appear three options:\n",
    "\n",
    "1. Execute a new model. execute_new_model()\n",
    "2. Reformat the dataset. change_date_to_one_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    OPTIONS MENU\n",
    "    Allows to select differents tasks:\n",
    "        1. Execute model will allow you to train a new model\n",
    "        2. Change the dataset will allow you to reformat the dataset so it can run using models\n",
    "    \"\"\"\n",
    "    options = 'What would you like to do?'\n",
    "    options += '\\n Enter 1 to execute a new model'\n",
    "    options += '\\n Enter 2 to format the dataset'\n",
    "    options += '\\n Your option: '\n",
    "    choice = eval(input(options))\n",
    "    \n",
    "    if choice == 1:\n",
    "        print(\"You selected to execute a new model\")\n",
    "        execute_new_model()\n",
    "    elif choice == 2:\n",
    "        print(\"You decided to reformat the dataset\")\n",
    "        change_date_to_one_column()\n",
    "    else:\n",
    "        print('You entered a wrong number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running a new model. Make sure you are using a valid dataset. If you go to change_date_to_one_column() function there is more information. At the beginning of this document, I also talked about how the dataset columns needs to be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to change parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the parameters when executing a new model. You need to go at execute_new_model() function.\n",
    "Make sure to follow the instruction when it comes to executing a new model or it might not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_new_model():\n",
    "    \n",
    "    ....\n",
    "    \n",
    "    neurons = 10\n",
    "    lookBack = 12\n",
    "    delay = 2\n",
    "    test_samples = 12     \n",
    "    n_epochs = 10\n",
    "    n_batch = 1\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't explored much the different combinations. I am sure the model can give better results by tunning the above parameters (or adding more layers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
